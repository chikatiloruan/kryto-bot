# bot/forum_tracker.py
import threading
import time
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional
from urllib.parse import urljoin
from .utils import (
    normalize_url, detect_type,
    extract_thread_id, extract_post_id_from_article, log_infO, log_error
)
from .storage import list_all_tracks, update_last
import traceback
import datetime

# ======================================================================
#   –ì–õ–ê–í–ù–´–ô –§–ò–ö–°:
#   - –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ cookies
#   - domain=".matrp.ru"
#   - –Ω–æ—Ä–º–∞–ª—å–Ω—ã–π User-Agent
#   - debug –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ
#   - —Ñ–∏–∫—Å—ã –ø–æ—Å—Ç–∏–Ω–≥–∞ message_html
# ======================================================================

try:
    from config import XF_USER, XF_SESSION, XF_TFA_TRUST, FORUM_BASE, POLL_INTERVAL_SEC
except Exception:
    XF_USER = ""
    XF_SESSION = ""
    XF_TFA_TRUST = ""
    FORUM_BASE = ""
    POLL_INTERVAL_SEC = 20

DEFAULT_POLL = 20
try:
    POLL = int(POLL_INTERVAL_SEC)
    if POLL <= 0:
        POLL = DEFAULT_POLL
except Exception:
    POLL = DEFAULT_POLL


def debug(msg: str):
    """ –ö—Ä–∞—Å–∏–≤—ã–π timestamp debug """
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{now}] [DEBUG] {msg}")


def warn(msg: str):
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    print(f"[{now}] [WARNING] {msg}")


# ======================================================================
# COOKIE
# ======================================================================
def build_cookies() -> dict:
    """ –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç cookies –∫–∞–∫ —Å–ª–æ–≤–∞—Ä—å –¥–ª—è requests """
    return {
        "xf_user": XF_USER,
        "xf_session": XF_SESSION,
        "xf_tfa_trust": XF_TFA_TRUST,
    }


def fetch_html(url: str, timeout: int = 15) -> str:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ–º HTML —Å –ø—Ä–∞–≤–∏–ª—å–Ω—ã–º–∏ cookie + UA
    """
    if not url:
        return ""

    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
        "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8",
        "Referer": FORUM_BASE or "",
    }

    try:
        r = requests.get(url, headers=headers, cookies=build_cookies(), timeout=timeout)
        if r.status_code == 200:
            return r.text

        warn(f"HTTP {r.status_code} for {url}")
        return ""

    except Exception as e:
        warn(f"fetch_html error: {e}")
        return ""
# ======================================================================
#  –ü–ê–†–°–ï–†–´ ‚Äî —Ç–µ–º—ã, –ø–æ—Å—Ç—ã, —Ä–∞–∑–¥–µ–ª—ã
# ======================================================================

def parse_thread_posts(html: str, page_url: str) -> List[Dict]:
    soup = BeautifulSoup(html or "", "html.parser")

    # –ù–ê–°–¢–û–Ø–©–ò–ï –ø–æ—Å—Ç—ã XenForo 2.3 –Ω–∞ MatRP
    messages = soup.select("div.message[data-content]")

    out = []

    for m in messages:
        try:
            # ID –ø–æ—Å—Ç–∞
            pid = m.get("data-content", "").replace("post-", "")

            # –ê–≤—Ç–æ—Ä
            author_el = m.select_one(".message-name a, .username a")
            author = author_el.get_text(strip=True) if author_el else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

            # –î–∞—Ç–∞/–≤—Ä–µ–º—è
            time_el = m.select_one("time")
            date = time_el.get("datetime", "") if time_el else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

            # –¢–µ–∫—Å—Ç
            body = m.select_one(".bbWrapper")
            text = body.get_text("\n", strip=True) if body else ""

            # –°—Å—ã–ª–∫–∞
            link = f"{page_url}#post-{pid}"

            out.append({
                "id": pid,
                "author": author,
                "date": date,
                "text": text,
                "link": link
            })

        except Exception as e:
            warn(f"parse_thread_posts error: {e}")
            continue

    return out



def parse_forum_topics(html: str, page_url: str) -> List[Dict]:
    soup = BeautifulSoup(html or "", "html.parser")
    items = soup.select(".structItem--thread, .structItem, .discussionListItem, .threadbit")

    out = []
    for it in items:
        try:
            a = it.select_one(
                ".structItem-title a, a[href*='/threads/'], "
                "a[href*='index.php?threads='], a.thread-title"
            )
            if not a:
                continue

            href = a.get("href")
            full = href if href.startswith("http") else urljoin(FORUM_BASE + "/", href.lstrip("/"))

            tid = extract_thread_id(full)
            title = a.get_text(strip=True)

            author_el = it.select_one(".structItem-minor a, .username, .poster")
            author = author_el.get_text(strip=True) if author_el else "–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–æ"

            out.append({
                "tid": str(tid),
                "title": title,
                "author": author,
                "url": full
            })
        except Exception as e:
            warn(f"parse_forum_topics error: {e}")
            continue

    return out
# ======================================================================
#  –ö–õ–ê–°–° ForumTracker ‚Äî –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥, keepalive, –æ–±—Ä–∞–±–æ—Ç–∫–∞ –Ω–æ–≤—ã—Ö –ø–æ—Å—Ç–æ–≤
# ======================================================================

class ForumTracker:
    """
    –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç 2 –≤–∞—Ä–∏–∞–Ω—Ç–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏:
      - ForumTracker(vk)
      - ForumTracker(XF_USER, XF_TFA_TRUST, XF_SESSION, vk)
    """

    def __init__(self, *args):
        self.interval = POLL
        self._running = False
        self.vk = None
        self._keepalive_running = True

        self.session = requests.Session()
        self.session.headers.update({
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64)",
            "Accept": "*/*",
            "Referer": FORUM_BASE
        })

        # -------------------------
        # –°–•–ï–ú–ê 1 ARG ‚Äî ForumTracker(vk)
        # -------------------------
        if len(args) == 1:
            self.vk = args[0]

            # –∫—É–∫–∏ –±–µ—Ä—ë–º –∏–∑ config.py
            for k, v in build_cookies().items():
                if v:
                    self.session.cookies.set(k, v)

        # -------------------------
        # –°–•–ï–ú–ê 4 ARGS ‚Äî —Å—Ç–∞—Ä—ã–π –≤–∏–¥ ForumTracker(XF_USER, XF_TFA_TRUST, XF_SESSION, vk)
        # -------------------------
        elif len(args) >= 4:
            xf_user, xf_tfa_trust, xf_session, vk = args[:4]
            self.vk = vk

            # —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ –≥–ª–æ–±–∞–ª—ã
            globals()["XF_USER"] = xf_user
            globals()["XF_TFA_TRUST"] = xf_tfa_trust
            globals()["XF_SESSION"] = xf_session

            # —Å—Ç–∞–≤–∏–º –∫—É–∫–∏ –≤ session
            if xf_user:
                self.session.cookies.set("xf_user", xf_user, domain="forum.matrp.ru")
            if xf_tfa_trust:
                self.session.cookies.set("xf_tfa_trust", xf_tfa_trust, domain="forum.matrp.ru")
            if xf_session:
                self.session.cookies.set("xf_session", xf_session, domain="forum.matrp.ru")

        else:
            raise TypeError("ForumTracker expected (vk) or (XF_USER, XF_TFA_TRUST, XF_SESSION, vk)")

        # —Ç—Ä–∏–≥–≥–µ—Ä –¥–ª—è /check
        if hasattr(self.vk, "set_trigger"):
            try:
                self.vk.set_trigger(self.force_check)
            except:
                pass

        # –∑–∞–ø—É—Å–∫ keepalive
        threading.Thread(target=self._keepalive_loop, daemon=True).start()

    # ===================================================================
    # API —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è
    # ===================================================================

    def start(self):
        self._running = True
        threading.Thread(target=self._loop, daemon=True).start()
        log_info(f"ForumTracker started (interval={self.interval})")

    def stop(self):
        self._running = False
        self._keepalive_running = False
        log_info("ForumTracker stopped")

    def force_check(self):
        threading.Thread(target=self.check_all, daemon=True).start()

    # ===================================================================
    # –û—Å–Ω–æ–≤–Ω–æ–π —Ü–∏–∫–ª
    # ===================================================================
    def _loop(self):
        while self._running:
            try:
                self.check_all()
            except Exception as e:
                warn(f"Tracker loop error: {e}")
            time.sleep(self.interval)

    # ===================================================================
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –≤—Å–µ—Ö –ø–æ–¥–ø–∏—Å–æ–∫
    # ===================================================================
    def check_all(self):
        rows = list_all_tracks()
        if not rows:
            return

        # –≥—Ä—É–ø–ø–∏—Ä—É–µ–º –ø–æ URL
        by_url = {}
        for peer_id, url, typ, last_id in rows:
            by_url.setdefault(url, []).append((peer_id, typ, last_id))

        for url, subs in by_url.items():
            try:
                self._process_url(url, subs)
            except Exception as e:
                warn(f"_process_url error for {url}: {e}")
                traceback.print_exc()

    # ===================================================================
    # –õ–æ–≥–∏–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –æ–¥–Ω–æ–π —Å—Å—ã–ª–∫–∏
    # ===================================================================
    def _process_url(self, url: str, subs):
        url = normalize_url(url)
        html = fetch_html(url)

        if not html:
            warn(f"HTTP error / no HTML for {url}")
            return

        typ = detect_type(url)

     

    # ===================================================================
    # KEEPALIVE ‚Äî –ø–∏–Ω–≥ —Ñ–æ—Ä—É–º–∞ —Ä–∞–∑ –≤ N —Å–µ–∫—É–Ω–¥ (–¥–µ—Ä–∂–∏—Ç —Å–µ—Å—Å–∏—é –∞–∫—Ç–∏–≤–Ω–æ–π)
    # ===================================================================
    def _keepalive_loop(self):
        while self._keepalive_running:
            try:
                fetch_html(FORUM_BASE)
            except Exception as e:
                warn(f"keepalive error: {e}")
            time.sleep(max(60, self.interval * 3))

        # ---------------------------------------------------------------
        #     THREAD
        # ---------------------------------------------------------------
        if typ == "thread":
            posts = parse_thread_posts(html, url)
            if not posts:
                return

            newest = posts[-1]

            for peer_id, _, last in subs:
                last_str = str(last) if last is not None else None

                if last_str != newest["id"]:
                    # —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ
                    msg = (
                        "üìù –ù–æ–≤—ã–π –ø–æ—Å—Ç\n"
                        f"üë§ {newest['author']} ‚Ä¢ {newest['date']}\n\n"
                        f"{newest['text'][:1500]}\n\n"
                        f"üîó {newest['link']}"
                    )

                    try:
                        self.vk.send(peer_id, msg)
                    except Exception as e:
                        warn(f"VK send error: {e}")

                    update_last(peer_id, url, newest["id"])

        # ---------------------------------------------------------------
        #     FORUM (–Ω–æ–≤—ã–µ —Ç–µ–º—ã)
        # ---------------------------------------------------------------
        elif typ == "forum":
            topics = parse_forum_topics(html, url)
            if not topics:
                return

            latest = topics[-6:]

            for peer_id, _, last in subs:
                last_str = str(last) if last is not None else None

                for t in latest:
                    if last_str != t["tid"]:
                        msg = (
                            "üÜï –ù–æ–≤–∞—è —Ç–µ–º–∞\n"
                            f"üìÑ {t['title']}\n"
                            f"üë§ {t['author']}\n"
                            f"üîó {t['url']}"
                        )
                        try:
                            self.vk.send(peer_id, msg)
                        except:
                            pass

                        update_last(peer_id, url, t["tid"])

        # ---------------------------------------------------------------
        #     MEMBERS
        # ---------------------------------------------------------------
        elif typ == "members":
            soup = BeautifulSoup(html, "html.parser")
            users = [a.get_text(strip=True) for a in soup.select(".username")[:20]]
            s = "üë• –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–∏: " + ", ".join(users)

            for peer_id, _, _ in subs:
                try:
                    self.vk.send(peer_id, s)
                except:
                    pass


        # ===================================================================
    #  –†–£–ß–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –í–°–ï–• –ü–û–°–¢–û–í (–¥–ª—è /checkfa) ‚Äî FIXED
    # ===================================================================
    def manual_fetch_posts(self, url: str):
        url = normalize_url(url)

        debug(f"[manual_fetch_posts] URL = {url}")
        debug(f"[manual_fetch_posts] Cookies = {build_cookies()}")

        if not url.startswith(FORUM_BASE):
            return {"ok": False, "error": "URL outside FORUM_BASE"}

        html = fetch_html(url)
        if not html:
            return {"ok": False, "error": "Cannot fetch page"}

        posts = parse_thread_posts(html, url)

        debug(f"[manual_fetch_posts] Parsed posts = {len(posts)}")

        return {"ok": True, "posts": posts}

    # ===================================================================
    # DEBUG: —á—Ç–æ –±–æ—Ç –≤–∏–¥–∏—Ç –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ ‚Äî FIXED
    # ===================================================================

    def debug_reply_form(self, url: str):
        url = normalize_url(url)
        html = fetch_html(url)

        cookies = build_cookies()

        if not html:
            return (
                "‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å —Å—Ç—Ä–∞–Ω–∏—Ü—É\n"
                f"Cookies: {cookies}"
            )

        soup = BeautifulSoup(html, "html.parser")

        form = (
            soup.select_one("form[action*='add-reply']") or
            soup.select_one("form.js-quickReply") or
            soup.select_one("form[data-xf-init*='quick-reply']") or
            soup.select_one("form[action*='post']")
        )

        textarea = None
        if form:
            textarea = (
                form.select_one("textarea[name='message_html']") or
                form.select_one("textarea[name='message']") or
                form.select_one("textarea")
            )

        logged = (
            "LogOut" in html or 
            "–í—ã–π—Ç–∏" in html or 
            "account" in html or
            "data-xf-init=\"member-tooltip\"" in html
        )

        return (
            "üîç DEBUG REPLY FORM\n"
            f"‚úî Logged in: {logged}\n"
            f"‚úî Cookies OK: {bool(cookies)}\n"
            f"‚úî Form found: {bool(form)}\n"
            f"‚úî Textarea found: {bool(textarea)}\n"
            f"‚úî Textarea name: {textarea.get('name') if textarea else '‚Äî'}\n"
            f"‚úî Action: {form.get('action') if form else '‚Äî'}\n"
            "-----------------------------------\n"
            "Cookies:\n"
            f"{cookies}\n"
            "-----------------------------------\n"
            "HTML —Å–Ω–∏–∑—É —Å—Ç—Ä–∞–Ω–∏—Ü—ã:\n"
            + html[-2000:]
        )
        

def post_message(self, url: str, message: str):
    """
    –ü–æ–ª–Ω–æ—Å—Ç—å—é –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–Ω–∞—è –æ—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç–≤–µ—Ç–∞ –≤ —Ç–µ–º—É MatRP (XenForo 2.3).
    –†–∞–±–æ—Ç–∞–µ—Ç –Ω–∞ –≤—Å–µ—Ö —Ç–µ–º–∞—Ö, –≤–∫–ª—é—á–∞—è –Ω–æ–≤—ã–µ, —Å—Ç–∞—Ä—ã–µ –∏ –ø—Ä–∏–≤–∞—Ç–Ω—ã–µ.
    """

    debug(f"[POST] Sending to: {url}")

    url = normalize_url(url)
    if not url.startswith(FORUM_BASE):
        return {"ok": False, "error": "URL outside FORUM_BASE"}

    # ---- –ø—Ä–æ–≤–µ—Ä—è–µ–º –∫—É–∫–∏ ------------------------------------------------------
    debug(f"[POST] Cookies: xf_user={XF_USER[:6]}..., xf_session={XF_SESSION[:6]}..., xf_tfa={XF_TFA_TRUST[:6]}...")

    html = fetch_html(url)
    if not html:
        return {"ok": False, "error": "Cannot fetch page"}

    soup = BeautifulSoup(html, "html.parser")

    # ---- 1) –∏—â–µ–º —Ñ–æ—Ä–º—É -------------------------------------------------------
    form = (
        soup.select_one("form[action*='add-reply']")
        or soup.select_one("form.js-quickReply")
        or soup.select_one("form[data-xf-init*='quick-reply']")
        or soup.select_one("form[action*='post']")
    )

    debug(f"[POST] Form found: {bool(form)}")

    if not form:
        return {"ok": False, "error": "Reply form not found"}

    action = form.get("action") or url
    if not action.startswith("http"):
        action = urljoin(FORUM_BASE, action.lstrip("/"))

    debug(f"[POST] Form action: {action}")

    # ---- 2) hidden –ø–æ–ª—è ------------------------------------------------------
    payload = {}

    for inp in form.select("input"):
        name = inp.get("name")
        if name:
            payload[name] = inp.get("value", "")

    # XenForo –ø–æ–ª—è
    payload["_xfWithData"] = "1"
    payload["_xfResponseType"] = "json"

    # TOKEN
    token = payload.get("_xfToken")
    if not token:
        t = soup.find("input", {"name": "_xfToken"})
        if t:
            payload["_xfToken"] = t.get("value", "")

    debug(f"[POST] xfToken: {payload.get('_xfToken')}")

    # _xfRequestUri = URL –≤–Ω—É—Ç—Ä–∏ —Ñ–æ—Ä—É–º–∞
    try:
        payload["_xfRequestUri"] = url.replace(FORUM_BASE, "")
        if not payload["_xfRequestUri"]:
            payload["_xfRequestUri"] = "/"
    except:
        payload["_xfRequestUri"] = "/"

    # ---- 3) –ø–æ–ª–µ message -----------------------------------------------------
    textarea = (
        form.select_one("textarea[name='message_html']")
        or form.select_one("textarea[name='message']")
        or form.select_one("textarea[data-original-name='message']")
    )

    debug(f"[POST] Textarea found: {bool(textarea)}")

    if not textarea:
        return {"ok": False, "error": "Textarea not found"}

    textarea_name = textarea.get("name")

    html_msg = f"<p>{message}</p>"

    # XenForo —Ç—Ä–µ–±—É–µ—Ç –æ–±–∞ –ø–æ–ª—è
    payload[textarea_name] = html_msg
    payload["message"] = message
    payload["message_html"] = html_msg

    # ---- 4) –ó–∞–≥–æ–ª–æ–≤–∫–∏ --------------------------------------------------------
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:132.0) Gecko/20100101 Firefox/132.0",
        "Referer": url,
        "X-Requested-With": "XMLHttpRequest",
        "Accept": "*/*",
    }

    # ---- 5) –û–±—ã—á–Ω—ã–π POST -----------------------------------------------------
    debug("[POST] Trying normal mode...")

    try:
        r = self.session.post(action, data=payload, headers=headers)
        debug(f"[POST] Normal POST code: {r.status_code}")

        if r.status_code in (200, 204, 302):
            time.sleep(1)
            check = fetch_html(url)
            if message.split()[0] in check:
                return {"ok": True, "response": "posted (normal)"}

        normal_error = f"HTTP {r.status_code}"
    except Exception as e:
        normal_error = str(e)

    warn(f"[POST] Normal failed: {normal_error}")

    # ---- 6) MULTIPART fallback ----------------------------------------------
    debug("[POST] Trying multipart...")

    multipart = {
        textarea_name: (None, html_msg, "text/html"),
        "message": (None, message),
        "message_html": (None, html_msg),
    }

    for k, v in payload.items():
        if k not in multipart:
            multipart[k] = (None, v)

    try:
        r = self.session.post(action, files=multipart, headers=headers)
        debug(f"[POST] Multipart code: {r.status_code}")

        if r.status_code in (200, 204, 302):
            time.sleep(1)
            check = fetch_html(url)
            if message.split()[0] in check:
                return {"ok": True, "response": "posted (multipart)"}

        multipart_error = f"HTTP {r.status_code}"
    except Exception as e:
        multipart_error = str(e)

    warn(f"[POST] Multipart failed: {multipart_error}")

    # ---- 7) fail -------------------------------------------------------------
    return {
        "ok": False,
        "error": "Post failed",
        "normal_err": normal_error,
        "multipart_err": multipart_error
    }

    def check_cookies(self):
        """
        –ü–æ–ª–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ —á–µ—Ä–µ–∑ cookies.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç True –µ—Å–ª–∏ –±–æ—Ç —Ä–µ–∞–ª—å–Ω–æ –∑–∞–ª–æ–≥–∏–Ω–µ–Ω –Ω–∞ —Ñ–æ—Ä—É–º–µ.
        """

        test_url = FORUM_BASE + "/index.php"
        
        headers = {
            "User-Agent": (
                "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
                "AppleWebKit/537.36 (KHTML, like Gecko) "
                "Chrome/122.0.0.0 Safari/537.36"
            )
        }

        # cookies, –∫–æ—Ç–æ—Ä—ã–µ —Ç—ã –∏—Å–ø–æ–ª—å–∑—É–µ—à—å
        cookies = {
            "xf_user": XF_USER,
            "xf_session": XF_SESSION,
            "xf_tfa_trust": XF_TFA_TRUST
        }

        try:
            r = self.session.get(test_url, headers=headers, cookies=cookies)
            html = r.text

            # –ø—Ä–∏–∑–Ω–∞–∫ –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏–∏ ‚Äî –µ—Å—Ç—å –∫–Ω–æ–ø–∫–∞ "–í—ã–π—Ç–∏"
            logged = ("logout" in html.lower()) or ("–≤—ã–π—Ç–∏" in html.lower())

            return {
                "ok": True,
                "logged_in": logged,
                "status": r.status_code,
                "cookies_sent": cookies,
                "html_sample": html[:500]
            }

        except Exception as e:
            return {"ok": False, "error": str(e)}


# ======================================================================
#  –û–°–¢–ê–í–ê–¢–¨–°–Ø –û–ù–õ–ê–ô–ù (–§–£–ù–ö–¶–ò–Ø –î–õ–Ø main.py)
# ======================================================================

def stay_online_loop():
    """
    –ö–∞–∂–¥—ã–µ 3 –º–∏–Ω—É—Ç—ã –ø–∏–Ω–≥—É–µ–º —Ñ–æ—Ä—É–º, —á—Ç–æ–±—ã –∞–∫–∫–∞—É–Ω—Ç –±—ã–ª '–û–Ω–ª–∞–π–Ω'.
    """
    import requests
    from .forum_tracker import build_cookies, FORUM_BASE
    import time

    cookies = build_cookies()
    url = FORUM_BASE or ""

    if not url:
        print("[ONLINE] FORUM_BASE –ø—É—Å—Ç–æ–π ‚Äî keepalive –≤—ã–∫–ª—é—á–µ–Ω")
        return

    while True:
        try:
            requests.get(url, cookies=cookies, timeout=10)
            print("[ONLINE] ping OK")
        except Exception as e:
            print("[ONLINE ERROR]", e)
        time.sleep(180)
